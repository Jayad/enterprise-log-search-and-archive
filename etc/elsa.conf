{
	# Plugin configuration.  Disabled by default.
	"plugins": {
            #"SNORT": "Info::Snort",
            #"WINDOWS": "Info::Windows",
            #"URL": "Info::Url"
    },
    "info": {
            #"snort": {
            #        "rules_file": "/usr/local/elsa/conf/emerging-all.rules"
            #},
            #"windows": {
            #        "eventids.csv": "/usr/local/elsa/conf/eventids.csv",
            #        "link_templates": [
            #                "http://www.ultimatewindowssecurity.com/securitylog/encyclopedia/event.aspx?eventid=%d"
            #        ]
            #}
    },
    # How many archive queries can be run simultaneously by all users?  (Can slow system)
    "max_concurrent_archive_queries": 4,
    # How often are queries run for alerts?  Default should be fine here.
    "schedule_interval" : 60,
    # Settings for email alerts
	"email": {
		"display_address": "noreply-elsa@example.com",
		"base_url" : "http://elsa/",
		"subject": "ELSA Alert"
	},
	# Secret that is used to create the hash for the result permalinks.  Change it to anything you want, but after you've issued a link, changing it will invalidate that link in the future.
	"link_key" : "secret",
	"yui" : {
		#"local" : "inc/combo.js",
		"version" : "2.8.1",
		"modifier" : ""
	},
	# Settings for connecting to the log backend system(s) 
	"cluster" : {
            "server" : "localhost",
            "port" : 8000,
            "kernel" : "syslog:8000",
            "timeout" : 10
    },
    # Settings for the user to run the Janus middleware script on the front-end server.  This could theoretically be run on its own server, but there's no real reason for doing that.
    "Janus" : {
    	"user": "www-data",
    	"group": "www-data",
    	"pidfile": "/usr/local/elsa/web/janus.pid",
		"server" : "localhost",
		"port" : 8001,
        "kernel" : "Janus",
        "session" : "webserver",
		"timeout" : 15,
		"dispatch_frequency" : 5
    },
    # This DB stores the query log, user permissions, etc. for the frontend.
    "meta_db" : {
            "dsn" : "dbi:mysql:database=syslog_web",
            "username" : "root",
            "password" : ""
    },
    # Auth method.  Values can be "none," "local," and "ldap." 
    "auth": {
		"method" : "local"
    },
    # Groups that define who are admins.  These are local groups by default, but can be LDAP groups if using LDAP auth.
    "admin_groups" : [ "system", "admin" ],
    # Trivial directoroies where the system will coordinate locks.  Default should always be fine unless you change from "/usr/local/elsa"
	"lockfile_dir": "/usr/local/elsa/node/tmp/locks",
    "query_cancel_dir": "/usr/local/elsa/node/tmp",
    # Tuning for number of indexes the system will handle.  There are temporary indexes which are (by default) 60 second batch-loaded indexes which get consolidated when the total number of logs in temporary indexes is over the setting for "perm_index_size."  By default, ELSA will consolidate its temporary indexes when it hits 10 million or temporary indexes are more than 40% of 200 (80).  If 200 permanent indexes are used before it hits the log size capacity, then it will prematurely overwrite permanent indexes.  Therefore, if you do not collect 10 million logs in less than 200 minutes, you will lose logs because they are overwritten prematurely.  So, if your logging rate is less than 833.3 logs per second, you will want to increase the number of indexes.
    "num_indexes": 200,
    # This is the offset for spacing of log ID numbers between peers.  You should not need to change this number (until one node his 1 trillion logs).
    "peer_id_multiplier" : 1000000000000,
    # Actively wipe known-empty indexes to make sure we're claiming all disk space we can between rotations.  You should leave this at 1.
    "validate_directory" : 1,
    # If you are running a cluster of backend nodes, list all of the peers here.  The value should be the server ID of each node (1, 2, 3...)
    "peers" : {
            #"192.168.12.23" : 1,
            #"192.168.23.23" : 2,
    },
    # If you want to archive logs in addition to indexing them, leave this and/or edit the numbers.  Set percentage to 0 if you do not want to archive.
    "archive": {
        "percentage": 33,
        "perm_index_size": 10000000,
    },
    # Size limit for logs + index size.  Set this to be 90-95% of your total data disk space.
    "log_size_limit" : 200000000000,
    "sphinx" : {
    	# location for sphinx indexer, should not need to change unless you installed to a non-standard location
    	"indexer": "/usr/local/bin/indexer",
    	# emergency consolidation will be done if too many temp indexes pile up
        "allowed_temp_percent" : 40,
        # Check to be sure that temporary indexes (which use a lot of RAM) never go above this amount of RAM, or they will be prematurely consolidated.
        "allowed_mem_percent": 25,
        # This should not need to be changed unless you change the ELSA root dir
        "config_template_file" : "/usr/local/elsa/node/conf/sphinx.conf.template",
        # Host/port sphinx runs on.  Default should be fine.
        "host" : "127.0.0.1",
        "port" : 3312,
        # The actual config file sphinx will use, should not need to change unless you are installing to non-standard locations.
        "config_file" : "/usr/local/etc/sphinx.conf",
        # Data directory for storing indexes.  Place on your biggest storage.
        "index_path" : "/data/sphinx",
        # This is the size of the temporary index batches.  You should probably leave as the default, but you can increase for fewer temp indexes, especially if you have low log volume.  The higher the value here, the longer it will take for new logs to be available to queries.
        "index_interval" : 60,
        # The pid file sphinx will use, should not need to change unless you are installing to non-standard locations.
        "pid_file" : "/data/sphinx/log/searchd.pid",
        # Setting for how many temp logs should be allowed before being consolidated.  This value should be fine for systems with more than 833 logs/second.
        "perm_index_size" : 10000000,
    },
    # Where to place the ELSA management logs (not the logs we are receiving)
    "logdir" : "/usr/local/elsa/log",
    # The agent spawns child processes.  This controls the concurrency of who can do what.  These settings should work fine for systems with 4 or more CPU's.
    "num_workers" : {
        "web": 4,
        "manager": 3
    },
    # Settings for the agent.pl process.  There should be a different server_id on each logging node.  Other values should not be changed.
    "manager" : {
        "server_id" : 0,
        "listen_port" : 8000,
        "server_name" : "127.0.0.1",
        "retry_time" : 5,
        "user" : "root",
        "group" : "root",
        "pidfile" : "/var/run/elsa.pid",
        "query_timeout" : 300,
    },
    # How many concurrent indexers can run. 1 should be fine.
    "num_indexers" : 1,
    # How many seconds the logs should be stored in each table.  Default should be fine.
    "table_interval" : 86400,
    # Logging level management logs (does not affect logs received).
    "debug_level" : "DEBUG",
    # Where to store the incoming logs before being loaded and processed.  You can set to /dev/shm on Linux for faster loading, but shouldn't be necessary.
    "buffer_dir" : "/data/elsa/tmp/buffers/",
    # Backend database for logs on backend nodes.  Set password if necessary.
    "database" : { 
            "dsn" : "dbi:mysql:database=syslog;", 
            "username" : "root", 
            "password" : "" 
    }
}